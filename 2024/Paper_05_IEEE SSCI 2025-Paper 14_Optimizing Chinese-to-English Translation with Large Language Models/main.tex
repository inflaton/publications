\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{url}
\usepackage{hyperref}
\usepackage{array}
\usepackage{tabularx}
\usepackage{balance}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{CJKutf8}
\usepackage{minted}
\usepackage{caption}
\usepackage{flushend}
\usepackage{pbalance}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
% Set UTF-8 encoding for the lstlisting environment
\lstset{inputencoding=utf8, 
        breaklines=true, 
        basicstyle=\footnotesize\ttfamily, 
        xleftmargin=0pt, 
        framexleftmargin=0pt, 
        framesep=0pt, 
        aboveskip=0pt, 
        belowskip=0pt, 
        showspaces=false, 
        showstringspaces=false,
        keepspaces=true,
        breakindent=0pt,
        numbersep=5pt,
        tabsize=4,
        keywordstyle=\color{blue}\bfseries
        }

\title{Optimizing Chinese-to-English Translation Using Large Language Models}
% \author{Anonymous Submission}

% \iffalse

\author{\IEEEauthorblockN{Donghao HUANG\textsuperscript{1,2},
Zhaoxia WANG\textsuperscript{1}}
\IEEEauthorblockA{\textsuperscript{1}School of Computing and Information Systems, Singapore Management University, Singapore}
\IEEEauthorblockA{\textsuperscript{2}Research and Development, Mastercard, Singapore}
\IEEEauthorblockA{dh.huang.2023@smu.edu.sg; zxwang@smu.edu.sg}
% \thanks{\IEEEauthorrefmark{1} Corresponding Author: Zhaoxia WANG (e-mail: zxwang@smu.edu.sg)}
}

% \fi

\maketitle

\begin{abstract}
The advent of Large Language Models (LLMs) has significantly advanced Chinese-to-English translation tasks. However, the translation process remains challenging due to the substantial differences in syntax, semantics, and morphology between these two languages, despite notable achievements. This paper presents a comprehensive study on Chinese-to-English translation, evaluating the performance of various LLMs. We explore a range of open-source models, from 3.5 billion to 72 billion parameters, and OpenAI’s latest models, across zero-shot, few-shot, and fine-tuned learning paradigms. Our analysis assesses translation quality using the COMET metric, reliability with the Translation Completeness Ratio (TCR), and efficiency via Characters per Second (CPS). The results highlight substantial trade-offs between model size, translation accuracy, and processing speed. Larger models tend to produce higher-quality translations, whereas smaller models offer greater efficiency. Fine-tuning significantly improves the performance of open-source LLMs, surpassing few-shot learning in both translation quality and processing speed. Proprietary models like GPT-4o exhibit consistent high performance without significant gains from fine-tuning. We emphasize the potential of fine-tuning with techniques like LoRA/QLoRA to optimize the balance between translation accuracy and computational efficiency, offering valuable insights for deploying LLMs in real-world translation scenarios.
\end{abstract}

\begin{IEEEkeywords}
Machine Translation, Large Language Models, Natural Language Processing, Chinese-English Translation, COMET Metric, Fine-tuning, Efficiency Analysis, Zero-shot Learning, Few-shot Learning
\end{IEEEkeywords}

\section{Introduction}

Machine Translation (MT) has been a pivotal area of research in Natural Language Processing (NLP), aiming to bridge communication gaps between speakers of different languages. The translation of Chinese to English poses particular challenges due to the significant differences in syntax, semantics, and morphology between these two languages. Traditional approaches, such as Rule-Based and Statistical Machine Translation (SMT), have relied on handcrafted rules or statistical patterns derived from parallel corpora \cite{koehn2009statistical}. However, 
%these methods often struggle with the complexities inherent in Chinese-English translation, such as handling idiomatic expressions, word order differences, and character-based text segmentation.
these methods often struggle with complexities inherent in Chinese-English translation, particularly in handling idiomatic expressions, word order differences, and character-based text segmentation.

The development of Neural Machine Translation (NMT) marked a significant shift, enabling the automatic learning of translation patterns from data \cite{sutskever2014sequence}. The introduction of the Transformer architecture \cite{vaswani2017attention} further advanced this field by leveraging self-attention mechanisms, leading to significant improvements in translation quality and efficiency. More recently, Large Language Models (LLMs), such as GPT-3 \cite{brown2020language} and mBERT \cite{devlin2018bert}, have demonstrated substantial potential in MT tasks, including zero-shot and few-shot translation capabilities.

This paper explores the application of state-of-the-art LLMs to Chinese-to-English translation, evaluating open-source models (3.5B–72B parameters) and OpenAI’s proprietary GPT-4o and GPT-4o-mini. This diverse selection highlights the capabilities of both open-source and cutting-edge proprietary models in machine translation.

%This paper investigates the application of state-of-the-art LLMs to Chinese-to-English translation, with a focus on their performance across different learning paradigms. We conducted a comprehensive evaluation of leading open-source models, ranging from 3.5B to 72B parameters, as well as OpenAI’s latest models: GPT-4o and GPT-4o-mini.
%This diverse selection enables us to explore the capabilities of both open-source LLMs and cutting-edge proprietary models within the context of machine translation.

Our work makes the following key contributions:

\begin{enumerate}
    \item \textbf{Comprehensive Evaluation:} We benchmark various LLMs (3.5B–72B parameters), comparing proprietary models (e.g., GPT-4o) and open-source ones (e.g., Llama, Qwen) for Chinese-to-English translation tasks.
    
    \item \textbf{Multi-metric Analysis:} We assess translation quality using COMET, reliability through the Translation Completeness Ratio (TCR), and efficiency using Characters per Second (CPS).

    \item \textbf{Learning Paradigm Comparison:} We examine zero-shot, few-shot, and fine-tuned performance, showing that fine-tuning significantly improves both quality and speed, especially for open-source models.

    \item \textbf{Fine-tuning Insights:} We provide a detailed analysis of the fine-tuning process, including performance improvements and optimal epochs for different models.

    \item \textbf{Efficiency-Quality Trade-off:} Our study explores the balance between translation quality and processing speed, highlighting the benefits of techniques like LoRA and QLoRA for efficient LLM applications.
    
    \item \textbf{Proprietary Model Observations:} We offer insights into proprietary models such as GPT-4o, which exhibit distinct behaviors in fine-tuning compared to open-source models.

    \item \textbf{Data and Code Transparency}: To promote reproducibility, all data and code associated with this study are made publicly available on GitHub: \href{https://github.com/inflaton-ai/translation}{https://github.com/inflaton-ai/translation}.
\end{enumerate}

\section{Related Work}

\subsection{Evolution of Machine Translation}

The field of Machine Translation (MT) has seen significant evolution, from Statistical Machine Translation (SMT) \cite{koehn2009statistical} to Neural Machine Translation (NMT) \cite{sutskever2014sequence}. The introduction of the Transformer architecture \cite{vaswani2017attention} marked a pivotal moment, setting new benchmarks in translation quality and efficiency. This progress has been particularly impactful for challenging language pairs like Chinese-English, where linguistic differences pose substantial hurdles \cite{zhang2017improving}.

\subsection{Large Language Models in Machine Translation}

Recent years have witnessed the rise of Large Language Models (LLMs) in MT tasks. Models like BERT \cite{devlin2018bert}, GPT-3 \cite{brown2020language}, and T5 \cite{raffel2020exploring} have demonstrated impressive capabilities in zero-shot and few-shot translation settings \cite{liu2020multilingual}. For Chinese-to-English translation specifically, models such as mBERT and XLM-R have leveraged cross-lingual representations to enhance translation quality \cite{conneau2020unsupervised}.

The application of LLMs to MT has opened new avenues for research:

\begin{itemize}
    \item \textbf{Zero-shot and Few-shot Learning:} Studies have explored the ability of LLMs to perform translations with minimal or no task-specific training \cite{brown2020language}.
    \item \textbf{Prompting Techniques:} Research has investigated various prompting strategies to optimize LLM performance in translation tasks \cite{zhang2023prompting}.
    \item \textbf{Fine-tuning Approaches:} Recent work has focused on efficient fine-tuning methods for adapting LLMs to specific translation tasks \cite{zhang2023machine}.
\end{itemize}

\subsection{Efficient Fine-tuning Techniques}

As LLMs grow in size, efficient fine-tuning becomes crucial. Two notable techniques have emerged:

\begin{itemize}
    \item \textbf{Low-Rank Adaptation (LoRA):} Introduced by Hu et al. \cite{hu2021lora}, LoRA significantly reduces the number of trainable parameters while maintaining model performance. It achieves this by adding low-rank decomposition matrices to the model's weight matrices.
    
    \item \textbf{Quantized LoRA (QLoRA):} Dettmers et al. \cite{dettmers2023qlora} extended LoRA by incorporating 4-bit quantization, further reducing memory requirements. This enables fine-tuning of very large models (70B+ parameters) on consumer-grade hardware.
\end{itemize}

These techniques have made it feasible to adapt large models to specific tasks efficiently, opening up new possibilities for specialized MT applications.

\subsection{Evaluation Metrics in Machine Translation and LLMs}

The evaluation of MT systems has also evolved, moving beyond traditional metrics like BLEU \cite{papineni2002bleu}. 
% Recent research has focused on metrics that better correlate with human judgments:

\begin{itemize}
    \item \textbf{COMET:} The Crosslingual Optimized Metric for Evaluation of Translation \cite{rei2022comet} utilizes contextual embeddings from pretrained language models to assess translation quality, demonstrating a strong correlation with human evaluations.
    
    \item \textbf{Efficiency Metrics:} Recent studies have increasingly emphasized the importance of efficiency metrics, such as inference speed, alongside traditional quality metrics when evaluating LLMs \cite{huang2024performance, huang2024automating}.
\end{itemize}

These efforts aim to enhance MT performance, making high-quality translation more accessible and efficient.
% These ongoing efforts aim to push the boundaries of MT performance while making high-quality translation more accessible and efficient for real-world applications.

\section{Dataset}

To evaluate the effectiveness of large language models in Chinese-to-English translation, we utilize the MAC (Manually Aligned Chinese-English) dataset, which is publicly available on GitHub\footnote{\url{https://github.com/bfsujason/mac}}. The MAC dataset is a carefully curated collection of parallel Mandarin-English texts, designed to support a range of NLP tasks, including machine translation, text classification, and linguistic analysis.

The dataset comprises sentences from six Chinese novels and their corresponding English translations, spanning a diverse set of genres such as humor, martial arts, classics, war, romance, and science fiction. To ensure representativeness, sentences are sampled from various sections of the novels.

For our experiments, we preprocess the dataset by first concatenating the Development (Dev) and Test sets into a single TSV file. We then prune entries that lack either Chinese or English paragraphs to ensure completeness. Subsequently, the dataset is split into training (80\%) and testing (20\%) sets. This preprocessing results in a total of 5,661 entries, with 4,528 entries in the training set (mac-train.tsv) and 1,133 entries in the testing set (mac-test.tsv).

This preprocessing ensures that the dataset is of high quality and reliability, making it suitable for robust training and evaluation in our experiments. By leveraging a diverse and representative sample of texts, we aim to comprehensively assess the performance of large language models in the challenging task of Chinese-to-English translation.

\section{Methodology}

\subsection{Selection of Large Language Models (LLMs)}

We selected a diverse set of state-of-the-art large language models (LLMs) for our experiments. This selection encompasses both open-source models with parameter counts ranging from 3.5 billion to 72 billion and the latest offerings from OpenAI, such as GPT-4o. For open-source models that do not natively support the Chinese language, we employed their fine-tuned Chinese versions. Table \ref{tab:ai_models} presents an overview of all the models utilized in our study, including their names, sizes (where applicable), and corresponding HuggingFace model identifiers for the open-source models.


\begin{table}[htbp]
\caption{Overview of Large Language Models and Their Specifications}
\label{tab:ai_models}
\centering
\resizebox{0.4895\textwidth}{!}{
\begin{tabular}{|l|l|r|l|}
\hline
\textbf{Company} & \textbf{Model Name} & \textbf{Size} & \textbf{HuggingFace Model ID} \\
\hline
\multirow{2}{*}{OpenAI} & GPT-4o & N/A & N/A \\
& GPT-4o-mini & N/A & N/A \\
\hline
\multirow{2}{*}{Meta} & Llama-3.1-8B & 8B & shenzhi-wang/Llama3.1-8B-Chinese-Chat \\
& Llama-3.1-70B & 70B & shenzhi-wang/Llama3.1-70B-Chinese-Chat \\
\hline
\multirow{2}{*}{Alibaba} & Qwen2-7B & 7B & Qwen/Qwen2-7B-Instruct \\
& Qwen2-72B & 72B & Qwen/Qwen2-72B-Instruct \\
\hline
Shanghai AI Laboratory & InternLM2.5-7B & 7B & internlm/internlm2\_5-7b-chat \\
\hline
Mistral AI & Mistral-7B & 7B & shenzhi-wang/Mistral-7B-v0.3-Chinese-Chat \\
\hline
Microsoft & Phi-3.5-mini & 3.5B & microsoft/Phi-3.5-mini-instruct \\
\hline
\end{tabular}
}
\end{table}

\subsection{Few-shot Learning and Parameter-Efficient Fine-tuning}

To enable the LLMs to perform translation tasks effectively, we employed few-shot prompting techniques. The system prompt used for translation is shown in Listing \ref{lst:llm-translation-prompt}.

\begin{lstlisting}[caption={System Prompt for LLM Translation}, 
                   label={lst:llm-translation-prompt}, 
                   breaklines=true, 
                   basicstyle=\footnotesize\ttfamily]
You are a helpful assistant that translates Chinese to English.
\end{lstlisting}

The user prompt template for translation is provided in Listing \ref{lst:llm-user-prompt}.

\begin{lstlisting}[caption={User Prompt Template for LLM Translation}, 
                   label={lst:llm-user-prompt}, 
                   breaklines=true, 
                   basicstyle=\footnotesize\ttfamily]
You will be given a Chinese sentence to translate. If it is an incomplete sentence, or if you are unsure about the meaning, simply copy the input text as your output. Do not output any additional sentence such as explanation or reasoning.
{exemplars}
Chinese: {input}
English:
\end{lstlisting}

The prompt template is designed to be a versatile tool for text translation. In our study, the placeholders were populated with specific data:

\begin{itemize}
    \item \texttt{\{exemplars\}}: For zero-shot translation, this field is left empty. For few-shot translation, exemplars are retrieved from the MAC training set and formatted as shown below:
\begin{CJK*}{UTF8}{gbsn}
\begin{minted}[fontsize=\footnotesize, 
               breaklines=true, 
               breakanywhere=true]{text}
Example Translations:
Chinese: 全仗着狐仙搭救。
English: Because I was protected by a fox fairy.
... (additional examples omitted for brevity)
\end{minted}
\end{CJK*}
    \item \texttt{\{input\}}: Original Chinese text for translation.
\end{itemize}

To enhance translation performance, we fine-tuned all models using the training set for five epochs. The details of the fine-tuning process are as follows:

\begin{itemize}
    \item \textbf{OpenAI's GPT-4o and GPT-4o-mini}: Fine-tuning was performed following OpenAI's official guidelines\footnote{\url{https://platform.openai.com/docs/guides/fine-tuning}}.
    \item \textbf{Llama-3.1-70B and Qwen2-72B variants}: Employed 4-bit quantization using LoRA (QLoRA) with the open-source library Llama Factory \cite{zheng2024llamafactory} on 4 Nvidia GPUs.
    \item \textbf{Other open-source models}: Used LoRA with the open-source library Llama Factory \cite{zheng2024llamafactory} on a single Nvidia GPU.
\end{itemize}

Utilizing these prompting techniques and fine-tuned adapters, we evaluated the performance of GPT-4o and GPT-4o-mini models via the OpenAI API, while the open-source models were evaluated on Nvidia L40 GPUs, , each equipped with 48GB of memory, using the open-source HuggingFace Transformers library\cite{wolf2020transformers}.

\subsection{Evaluation Metrics}

To assess the performance of each model, we employed multiple evaluation metrics to capture various aspects of translation quality and efficiency.
\subsubsection{COMET-22}

We employed the Crosslingual Optimized Metric for Evaluation of Translation (COMET), a neural-based evaluation metric that utilizes contextual embeddings from pretrained language models to assess translation quality. COMET has demonstrated a strong correlation with human judgments, establishing itself as a reliable metric for translation evaluation. The latest version, COMET-22, has been shown to be particularly effective in evaluating Chinese-to-English translations, outperforming traditional metrics such as BLEU and TER in terms of alignment with human evaluations \cite{rei2022comet, freitag2022results}. By using COMET-22, we aim to measure the semantic and contextual accuracy of translations produced by different models more objectively.

\subsubsection{Translation Completeness Ratio (TCR)}

During our evaluation, we observed that some outputs generated by the large language models (LLMs) contained untranslated segments with Chinese characters, resulting in incomplete translations. To quantify this issue, we introduced the Translation Completeness Ratio (TCR), which measures the proportion of translations that are fully rendered into the target language (English):

\begin{equation}
\text{TCR} = \frac{\text{Number of Complete Translations}}{\text{Total Number of Entries}}
\end{equation}

The TCR provides a metric for assessing the reliability of each model in consistently producing complete translations. A higher TCR value indicates fewer instances of untranslated content, reflecting better performance in maintaining translation completeness.

\subsubsection{Characters per Second (CPS)}

To evaluate the efficiency of the translation models in terms of processing speed, we introduced the Characters per Second (CPS) metric. This metric measures the number of Chinese characters translated per second, thereby reflecting the model’s computational efficiency:

\begin{equation}
\text{CPS} = \frac{\text{Number of Chinese Characters in the Dataset}}{\text{Total Time Used for Translation (seconds)}}
\end{equation}

The CPS metric is essential for understanding the practicality of each model in real-world applications where translation speed is a critical factor. A higher CPS value indicates faster translation speeds, which is particularly important for handling large volumes of text in time-sensitive scenarios.

\section{Results and Discussion}

Our study evaluated a diverse range of large language models for Chinese-to-English translation tasks, analyzing their performance across zero-shot, few-shot, and fine-tuned settings. We assessed these models using the COMET metric for translation quality, Translation Completeness Ratio (TCR) for reliability, and Characters per Second (CPS) for efficiency.

\subsection{Zero-shot and Few-shot Performance}

The performance of large language models in zero-shot and few-shot settings is summarized in Table~\ref{tab:model-performance}. This table presents the COMET scores, Translation Completeness Ratio (TCR), and Characters per Second (CPS) across various few-shot settings.

\begin{table*}[htbp]
\centering
\caption{Performance of Large Language Models Across Few-shot Settings}
\label{tab:model-performance}
\resizebox{0.52\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Metric} & \textbf{0-shot} & \textbf{1-shot} & \textbf{5-shot} & \textbf{10-shot} & \textbf{50-shot} \\
\hline
\multirow{3}{*}{GPT-4o} & COMET & 0.7258 & 0.7269 & 0.7172 & 0.7282 & \textbf{0.7362} \\
 & TCR & \textbf{0.9771} & 0.9612 & 0.8791 & 0.9365 & 0.9718 \\
 & CPS & 17.64 & \textbf{22.40} & 22.40 & 15.06 & 13.31 \\
\hline
\multirow{3}{*}{GPT-4o-mini} & COMET & 0.7259 & 0.7270 & 0.7178 & 0.7270 & \textbf{0.7371} \\
 & TCR & \textbf{0.9806} & 0.9603 & 0.8791 & 0.9303 & 0.9718 \\
 & CPS & 16.92 & 17.21 & 22.40 & \textbf{22.70} & 20.91 \\
\hline
\multirow{3}{*}{Qwen2-72B} & COMET & 0.7324 & 0.7379 & 0.7413 & 0.7437 & \textbf{0.7482} \\
 & TCR & 0.9673 & 0.9806 & 0.9859 & 0.9894 & \textbf{0.9903} \\
 & CPS & \textbf{3.03} & 3.00 & 1.57 & 0.91 & 0.24 \\
\hline
\multirow{3}{*}{Llama3.1-70B} & COMET & 0.7171 & 0.7264 & 0.7337 & 0.7355 & \textbf{0.7390} \\
 & TCR & 0.9515 & 0.9806 & 0.9806 & 0.9815 & \textbf{0.9929} \\
 & CPS & \textbf{3.44} & 3.25 & 1.56 & 1.56 & 0.23 \\
\hline
\multirow{3}{*}{Llama3.1-8B} & COMET & 0.7006 & 0.7085 & \textbf{0.7182} & 0.7156 & 0.7013 \\
 & TCR & 0.9859 & \textbf{0.9894} & 0.9779 & 0.9656 & 0.8959 \\
 & CPS & \textbf{25.08} & 23.30 & 9.58 & 9.58 & 0.64 \\
\hline
\multirow{3}{*}{Qwen2-7B} & COMET & 0.7170 & 0.7266 & 0.7350 & 0.7340 & \textbf{0.7386} \\
 & TCR & 0.9214 & 0.9241 & 0.9638 & 0.9762 & \textbf{0.9850} \\
 & CPS & \textbf{27.50} & 25.61 & 11.78 & 6.73 & 1.54 \\
\hline
\multirow{3}{*}{InternLM2.5-7B} & COMET & 0.7174 & 0.7185 & 0.7235 & \textbf{0.7256} & 0.7117 \\
 & TCR & \textbf{0.9912} & 0.9868 & 0.9876 & 0.9850 & 0.9594 \\
 & CPS & \textbf{22.03} & 20.54 & 9.43 & 4.71 & 0.64 \\
\hline
\multirow{3}{*}{Mistral-7B} & COMET & 0.6896 & 0.6940 & \textbf{0.7087} & 0.7019 & 0.7087 \\
 & TCR & 0.9841 & 0.9718 & 0.9850 & 0.9841 & \textbf{0.9912} \\
 & CPS & \textbf{22.38} & 18.20 & 7.96 & 4.11 & 0.91 \\
\hline
\multirow{3}{*}{Phi-3.5-mini} & COMET & 0.6638 & 0.6667 & 0.6795 & \textbf{0.6859} & 0.6781 \\
 & TCR & 0.9850 & 0.9620 & 0.9912 & \textbf{0.9938} & 0.9832 \\
 & CPS & \textbf{27.42} & 22.33 & 9.20 & 3.93 & 0.45 \\
\hline
\end{tabular}%
}
\end{table*}

The performance across zero-shot and few-shot settings reveals several notable trends: 

\begin{itemize}
    \item Zero-shot Performance:
    \begin{itemize}
        \item The Qwen2-72B model demonstrated the highest performance, achieving a COMET score of 0.7324.
        \item GPT-4o and GPT-4o-mini models followed closely, with COMET scores of 0.7258 and 0.7259, respectively.
        \item Smaller models, such as InternLM2.5-7B and Qwen2-7B, also showed competitive results, with COMET scores of 0.7174 and 0.7170, respectively.
    \end{itemize}

    \item Few-shot Learning:
    \begin{itemize}
        \item The Qwen2-72B model achieved the highest overall COMET score of 0.7482 in the 50-shot learning setting, showing a consistent upward trend as more examples were provided.
        \item The Llama3.1-70B model also showed significant improvement, reaching a score of 0.7390 with 50-shot learning, highlighting its capacity to learn effectively from additional examples.
        \item The Qwen2-7B model demonstrated impressive gains, attaining its peak score of 0.7386 with 50-shot learning, surpassing both the GPT-4o and GPT-4o-mini models.
        \item The GPT-4o and GPT-4o-mini models exhibited steady improvement across all few-shot settings, achieving their highest scores of 0.7362 and 0.7371, respectively, with 50-shot learning.
    \end{itemize}

    \item Translation Completeness Ratio (TCR):
    \begin{itemize}
        \item Most models maintained high TCR scores across different few-shot settings, often above 0.95, indicating consistent production of complete translations.
        \item Larger models generally showed more stable TCR scores across different few-shot settings.
    \end{itemize}

    \item Characters per Second (CPS):
    \begin{itemize}
        \item Smaller models consistently demonstrated higher CPS rates, with Phi-3.5-mini and Qwen2-7B achieving over 27 CPS in zero-shot settings.
        \item Larger models, while producing higher quality translations, showed significantly lower CPS rates. For instance, Qwen2-72B processed only 3.03 CPS in the zero-shot setting.
        \item CPS rates generally decreased as the number of few-shot examples increased, likely due to the increased computational load of processing additional examples.
    \end{itemize}
\end{itemize}

These results suggest that while there is a general trend of larger models yielding better performance in terms of translation quality (COMET scores), other factors such as model architecture and the quality of training data are also critical in determining overall effectiveness. The trade-off between translation quality and speed (CPS) is evident, with smaller models offering significantly higher processing speeds at the cost of somewhat lower quality. Few-shot learning generally improves performance across all models, demonstrating the adaptability of these models with limited training data.

\subsection{Fine-tuning Results}

Fine-tuning on a larger dataset led to notable improvements for most models. We observed important trends in the fine-tuning process over 5 epochs, as shown in Table~\ref{tab:fine-tuning-results}.

\begin{table*}[htbp]
\centering
\caption{Fine-tuning Results Across 0-5 Epochs}
\label{tab:fine-tuning-results}
\resizebox{0.65\textwidth}{!}{%
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Metric} & \textbf{0 epoch} & \textbf{1 epoch} & \textbf{2 epochs} & \textbf{3 epochs} & \textbf{4 epochs} & \textbf{5 epochs} \\
\hline
\multirow{3}{*}{GPT-4o} & COMET & 0.7258 & 0.7258 & 0.7260 & \textbf{0.7263} & 0.7257 & 0.7259 \\
 & TCR & \textbf{0.9771} & 0.9788 & 0.9788 & 0.9779 & 0.9788 & 0.9788 \\
 & CPS & 17.64 & 26.52 & 29.21 & 27.01 & 27.35 & \textbf{26.29} \\
\hline
\multirow{3}{*}{GPT-4o-mini} & COMET & 0.7259 & 0.7260 & 0.7259 & 0.7253 & \textbf{0.7264} & 0.7262 \\
 & TCR & \textbf{0.9806} & 0.9779 & 0.9762 & 0.9788 & 0.9788 & 0.9797 \\
 & CPS & 16.92 & 30.31 & 28.39 & 29.89 & 27.40 & \textbf{32.26} \\
\hline
\multirow{3}{*}{Qwen2-72B} & COMET & 0.7324 & 0.7552 & \textbf{0.7564} & 0.7475 & 0.7435 & 0.7365 \\
 & TCR & 0.9673 & 0.9991 & \textbf{1.0000} & 0.9982 & 0.9903 & 0.9788 \\
 & CPS & \textbf{3.03} & 1.78 & 1.74 & 1.73 & 1.67 & 1.62 \\
\hline
\multirow{3}{*}{Llama3.1-70B} & COMET & 0.7171 & 0.7386 & 0.7480 & \textbf{0.7495} & 0.7407 & 0.7361 \\
 & TCR & 0.9515 & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} \\
 & CPS & \textbf{3.44} & 1.76 & 1.76 & 1.68 & 1.66 & 1.66 \\
\hline
\multirow{3}{*}{Llama3.1-8B} & COMET & 0.7006 & 0.7235 & 0.7369 & \textbf{0.7431} & 0.7413 & 0.7384 \\
 & TCR & 0.9859 & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & 0.9991 & \textbf{1.0000} \\
 & CPS & \textbf{25.08} & 21.99 & 22.50 & 21.84 & 22.12 & 21.82 \\
\hline
\multirow{3}{*}{Qwen2-7B} & COMET & 0.7170 & 0.7319 & 0.7378 & \textbf{0.7474} & 0.7445 & 0.7391 \\
 & TCR & 0.9214 & 0.9938 & 0.9973 & 0.9646 & 0.9903 & \textbf{0.9894} \\
 & CPS & \textbf{27.50} & 22.97 & 21.35 & 21.24 & 21.84 & 21.36 \\
\hline
\multirow{3}{*}{InternLM2.5-7B} & COMET & 0.7174 & 0.7199 & 0.7292 & 0.7345 & \textbf{0.7397} & 0.7362 \\
 & TCR & 0.9912 & 0.9974 & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} \\
 & CPS & \textbf{22.03} & 17.55 & 17.22 & 16.87 & 17.32 & 17.09 \\
\hline
\multirow{3}{*}{Mistral-7B} & COMET & 0.6896 & 0.7103 & \textbf{0.7234} & 0.7212 & 0.7183 & 0.7119 \\
 & TCR & 0.9841 & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} \\
 & CPS & \textbf{22.38} & 20.35 & 19.58 & 19.65 & 18.98 & 18.52 \\
\hline
\multirow{3}{*}{Phi-3.5-mini} & COMET & 0.6638 & 0.6944 & 0.7028 & 0.7053 & 0.7111 & \textbf{0.7117} \\
 & TCR & 0.9850 & 0.9974 & \textbf{1.0000} & \textbf{1.0000} & \textbf{1.0000} & 0.9991 \\
 & CPS & \textbf{27.42} & 15.84 & 16.30 & 16.42 & 15.89 & 15.89 \\
\hline
\end{tabular}%
}
\end{table*}

Based on the fine-tuning results presented in Table~\ref{tab:fine-tuning-results}, we can make the following observations:

\begin{itemize}
    \item Performance Trends:
    \begin{itemize}
        \item Initially, most models showed improved performance as the number of fine-tuning epochs increased.
        \item However, some models began to show signs of performance plateau or slight decline within the 5 epochs, indicating the onset of overfitting.
    \end{itemize}

    \item Model-Specific Observations:
    \begin{itemize}
        \item The Qwen2-72B model achieved its peak performance after two epochs of fine-tuning, with a COMET score of 0.7564, representing a 3.28\% improvement over its zero-shot performance. After this peak, the scores began to decrease slightly.
        \item The Llama3.1-70B model showed improvement up to three epochs, reaching a score of 0.7495, a 4.51\% increase from its zero-shot performance. Performance plateaued after this point.
        \item Smaller models like Phi-3.5-mini showed a more extended improvement curve, with scores increasing up to the fifth epoch, reaching a peak COMET score of 0.7117, a 7.22\% improvement over its zero-shot performance.
        \item The Qwen2-7B model demonstrated significant improvement, peaking at 0.7474 after three epochs, a 4.24\% increase from its zero-shot score.
    \end{itemize}

    \item Epochs to Peak Performance:
    \begin{itemize}
        \item Larger models tended to reach their peak performance earlier, often within 2-3 epochs. For example, Qwen2-72B peaked at 2 epochs, and Llama3.1-70B at 3 epochs.
        \item Smaller models generally benefited from more epochs of fine-tuning within the 5-epoch limit. For instance, Phi-3.5-mini and InternLM2.5-7B showed their best performance at 5 and 4 epochs, respectively.
    \end{itemize}

    \item Translation Completeness Ratio (TCR):
    \begin{itemize}
        \item Most models showed improvement in TCR with fine-tuning, often reaching perfect or near-perfect scores (1.0000) after a few epochs.
        \item Larger models like Llama3.1-70B achieved and maintained perfect TCR from the first epoch onwards.
    \end{itemize}

\item Characters per Second (CPS):
\begin{itemize}
    \item Most models showed a decrease in CPS (characters per second) following fine-tuning, suggesting a trade-off between improved translation quality and processing speed. This reduction in speed could be attributed to the loading and inference of LoRA and QLoRA fine-tuned models. 
    \item LoRA and QLoRA adapters enable fine-tuning by adding small low-rank matrices to the original model weights, allowing models to adapt to new tasks with minimal changes to the pre-existing parameters. While this method is efficient in terms of memory and computation during training, the dynamic combination of pre-trained and adapter weights during inference may lead to reduced CPS.

    \item The GPT-4o and GPT-4o-mini models were notable exceptions, displaying an increase in CPS after fine-tuning. This suggests that their architecture or fine-tuning process may optimize the loading and merging of weights more effectively, reducing computational overhead and thereby enhancing processing speed.
\end{itemize}

    \item OpenAI Models Exception:
    \begin{itemize}
        \item Both GPT-4o and GPT-4o-mini showed no significant improvement in COMET scores during fine-tuning. Their scores remained essentially unchanged across all fine-tuning epochs.
        \item For GPT-4o, the COMET score changed minimally from 0.7258 (zero-shot) to 0.7259 (after 5 epochs), a negligible 0.01\% difference.
        \item Similarly, GPT-4o-mini's score went from 0.7259 (zero-shot) to 0.7264 (after 5 epochs), a mere 0.07\% increase.
        \item However, both models showed notable improvements in CPS after fine-tuning, with GPT-4o-mini achieving its highest CPS of 32.26 at 5 epochs.
    \end{itemize}
\end{itemize}

These results highlight the complex dynamics of fine-tuning large language models for translation tasks. While most models benefit from fine-tuning, the optimal number of epochs varies based on model size and architecture. The trade-off between translation quality, completeness, and speed becomes evident, with different models exhibiting unique patterns in these metrics across fine-tuning epochs.

\subsection{Comparison of Metrics Across Settings}

To better understand the impact of few-shot learning and fine-tuning on model performance, we compare the three key metrics (COMET, TCR, and CPS) for each model in three scenarios: baseline (0-shot), best few-shot, and best fine-tuned. Table~\ref{tab:metric-comparison} presents this comparison.

\begin{table*}[htbp]
\centering
\caption{Comparison of Metrics Across Baseline, Best Few-shot, and Best Fine-tuned Settings}
\label{tab:metric-comparison}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{|l|ccc|cccc|cccc|}
\hline
\multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{Baseline (0-shot)}} & \multicolumn{4}{c|}{\textbf{Best Few-shot}} & \multicolumn{4}{c|}{\textbf{Best Fine-tuned}} \\
\cline{2-12}
 & COMET & TCR & CPS & COMET & TCR & CPS & Shot & COMET & TCR & CPS & Epoch \\
\hline
GPT-4o & 0.7258 & 0.9771 & 17.64 & 0.7362 & 0.9718 & 13.31 & 50 & 0.7263 & 0.9788 & 29.21 & 2 \\
GPT-4o-mini & 0.7259 & 0.9806 & 16.92 & 0.7371 & 0.9718 & \textbf{20.91} & 50 & 0.7264 & 0.9797 & \textbf{32.26} & 5 \\
Qwen2-72B & \textbf{0.7324} & 0.9673 & 3.03 & \textbf{0.7482} & 0.9903 & 0.24 & 50 & \textbf{0.7564} & \textbf{1.0000} & 1.78 & 2 \\
Llama3.1-70B & 0.7171 & 0.9515 & 3.44 & 0.7390 & \textbf{0.9929} & 0.23 & 50 & 0.7495 & \textbf{1.0000} & 1.76 & 3 \\
Llama3.1-8B & 0.7006 & 0.9859 & 25.08 & 0.7182 & 0.9779 & 9.58 & 5 & 0.7431 & \textbf{1.0000} & 22.50 & 3 \\
Qwen2-7B & 0.7170 & 0.9214 & \textbf{27.50} & 0.7386 & 0.9850 & 1.54 & 50 & 0.7474 & 0.9973 & 22.97 & 3 \\
InternLM2.5-7B & 0.7174 & \textbf{0.9912} & 22.03 & 0.7256 & 0.9850 & 4.71 & 10 & 0.7397 & \textbf{1.0000} & 17.55 & 4 \\
Mistral-7B & 0.6896 & 0.9841 & 22.38 & 0.7087 & 0.9912 & 0.91 & 5 & 0.7234 & \textbf{1.0000} & 19.65 & 2 \\
Phi-3.5-mini & 0.6638 & 0.9850 & 27.42 & 0.6859 & 0.9938 & 3.93 & 10 & 0.7117 & \textbf{1.0000} & 16.42 & 5 \\
\hline
\end{tabular}%
}
\end{table*}

From this comparison, we can observe several trends:

\begin{itemize}
    \item COMET Scores:
    \begin{itemize}
        \item All models show improvement in COMET scores from baseline to their best performance, whether achieved through few-shot learning or fine-tuning.
        \item Larger models (Qwen2-72B, Llama3.1-70B) show the most significant improvements, with Qwen2-72B achieving the highest overall COMET score of 0.7564 after fine-tuning.
        \item Smaller models also show notable improvements, with Phi-3.5-mini demonstrating the largest relative improvement from baseline (0.6638) to best fine-tuned (0.7117), a 7.22\% increase.
        \item GPT-4o and GPT-4o-mini show minimal improvements, with their best performances achieved in the few-shot setting rather than through fine-tuning.
    \end{itemize}

    \item Translation Completeness Ratio (TCR):
    \begin{itemize}
        \item Most models achieve perfect or near-perfect TCR (1.0000) after fine-tuning, indicating highly reliable production of complete translations.
        \item Baseline TCR scores are already high for most models, suggesting that incomplete translations are not a significant issue even in zero-shot settings.
        \item The Qwen2-7B model shows the most significant improvement in TCR, from 0.9214 in the baseline to 0.9973 after fine-tuning.
    \end{itemize}

    \item Characters per Second (CPS):
    \begin{itemize}
        \item There's a notable trade-off between translation quality and speed, especially for larger models.
        \item Smaller models (e.g., Phi-3.5-mini, Qwen2-7B) generally maintain higher CPS rates across all settings compared to larger models.
        \item Larger models (Qwen2-72B, Llama3.1-70B) show significant decreases in CPS from baseline to best few-shot performance, but some recovery in the fine-tuned setting.
        \item Interestingly, GPT-4o and GPT-4o-mini show improvements in CPS after fine-tuning, with GPT-4o-mini achieving the highest overall CPS of 32.26 in its best fine-tuned state.
    \end{itemize}

    \item Overall Performance:
    \begin{itemize}
        \item Qwen2-72B demonstrates the best overall performance, achieving the highest COMET score (0.7564) and perfect TCR (1.0000) after fine-tuning, albeit with lower CPS.
        \item GPT-4o and GPT-4o-mini show strong baseline performance and maintain high CPS rates, but benefit less from few-shot learning and fine-tuning compared to other models.
        \item Smaller models like Qwen2-7B and InternLM2.5-7B show impressive improvements with fine-tuning, approaching the performance of larger models while maintaining higher CPS rates.
    \end{itemize}
\end{itemize}

This comparison highlights the complex interplay between model size, training approach, and performance across different metrics. While larger models generally achieve higher COMET scores, smaller models offer advantages in terms of processing speed. Notably, for all open-source Large Language Models (LLMs) in our study, fine-tuning not only leads to better COMET scores but also higher Characters per Second (CPS) rates compared to few-shot learning. This observation strongly suggests that fine-tuning is the recommended approach for translation tasks using these models.

The benefits of fine-tuning are particularly significant when considering the flexibility it offers. With techniques like Low-Rank Adaptation (LoRA) or its quantized version (QLoRA), the same LLM can be fine-tuned for various tasks, including translation. Different adapters can then be used for inference, creating a highly versatile solution for LLM applications. This approach allows for task-specific optimization without the need to retrain the entire model, making it both resource-efficient and adaptable to diverse requirements.

\section{Conclusion and Future Work}

Our study on large language models for Chinese-to-English translation provides key insights:

\begin{enumerate}
    \item Larger models (e.g., 72B parameters) achieve the best COMET scores, but architecture and training data significantly influence performance, with some 7B models showing competitive results.
    \item Fine-tuning improves both translation quality and efficiency for open-source models, outperforming few-shot learning and highlighting its importance for practical deployment.
    \item Metrics like COMET and TCR effectively evaluate translation quality, with fine-tuned models achieving high scores.
    \item Efficiency (CPS rates) varies greatly with model size: smaller models are faster, while larger models prioritize quality over speed.
    \item Optimal fine-tuning epochs depend on model size, with larger models peaking earlier (2--3 epochs) than smaller ones (4--5 epochs).
    \item OpenAI models (GPT-4o) show strong baseline performance with minimal gains from fine-tuning.
\end{enumerate}

These findings reveal trade-offs between model size, quality, and speed, with fine-tuning and techniques like LoRA/QLoRA offering promising solutions for efficient applications.

Future work could focus on:
\begin{enumerate}
    \item Multi-task learning approaches that combine translation with other NLP tasks.
    \item Hybrid systems leveraging smaller models for initial translations and larger ones for refinement.
    \item Extending analyses to other language pairs to assess the generalizability of the findings.
\end{enumerate}

In conclusion, while large language models excel in translation tasks, optimizing performance and efficiency remains a key challenge. Advancements in architecture, training techniques, and hybrid systems hold the potential to further improve translation quality and accessibility, fostering better global communication.

\iffalse
\section{Conclusion and Future Work}

Our comprehensive study of large language models for Chinese-to-English translation has yielded several key insights:

\begin{enumerate}
    \item Model size generally correlates with better translation quality, but architecture and training data also play crucial roles. Larger models (72B parameters) achieved the highest COMET scores, while some 7B parameter models showed competitive performance.

    \item Fine-tuning significantly improved both translation quality (COMET scores) and efficiency (CPS rates) for all open-source LLMs, outperforming few-shot learning. This suggests that fine-tuning should be the preferred approach for deploying these models in translation tasks.

    \item The COMET metric and Translation Completeness Ratio (TCR) proved valuable in evaluating translation quality, with most models achieving high scores, especially after fine-tuning.

    \item Efficiency, measured in Characters per Second (CPS), varied dramatically between model sizes. Smaller models demonstrated significantly higher CPS rates, while larger models, despite higher quality translations, were much slower.

    \item The optimal number of fine-tuning epochs varied based on model size and architecture, with larger models generally reaching peak performance earlier (2-3 epochs) compared to smaller models (4-5 epochs).

    \item The OpenAI models (GPT-4o and GPT-4o-mini) showed unique behavior, with minimal improvement from fine-tuning but maintaining high baseline performance and CPS rates.
\end{enumerate}

These findings highlight the complex trade-offs between model size, translation quality, and speed in real-world applications. The clear advantages of fine-tuning for open-source LLMs, combined with techniques like LoRA/QLoRA, present a promising direction for flexible and efficient LLM applications in machine translation.

Future work could explore:

\begin{enumerate}
    \item Exploring multi-task learning approaches combining translation with other NLP tasks.
        
    \item Developing hybrid systems that leverage the high CPS of smaller models for initial translation and use larger models for refinement.
    
    \item Extending this study to other language pairs to assess the generalizability of these findings.
\end{enumerate}

In conclusion, while large language models have shown impressive capabilities in Chinese-to-English translation, there remains significant room for improvement in balancing performance and efficiency. As the field progresses, we anticipate the development of models and techniques that can better optimize these competing factors, potentially through innovative architectures, more efficient training techniques, or hybrid systems. This continued research has the potential to further bridge language barriers and improve cross-cultural communication on a global scale, while making high-quality translation more accessible and efficient for a wide range of applications.
\fi

\bibliographystyle{IEEEtran}

% \IEEEtriggeratref{13}
\bibliography{references}

\end{document}
