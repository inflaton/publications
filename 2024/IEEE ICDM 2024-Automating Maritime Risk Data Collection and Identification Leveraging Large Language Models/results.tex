\section{Results and Discussions}

\subsection{Performance Evaluation: GMRID v1 vs. v2}
\label{subsec:v1_vs_v2}

We conducted classification using various machine learning models, including OpenAI's models with few-shot prompting, to determine their optimal performance settings. The best results of this evaluation are presented in Table \ref{tab:classification_model_evaluation_v2}.

\begin{table}[ht]
\caption{Classification Model Evaluation - Results Comparison Between GMRID v1 (Annotator-Labeled Ground Truth) and GMRID v2 (GPT-4o Labeled Ground Truth). This table presents the F1 scores of various machine learning models, including traditional approaches and LLMs, on both datasets. It demonstrates the performance improvement observed when using the GPT-4o labeled dataset (v2) compared to the human-labeled dataset (v1), suggesting potential benefits of LLM-generated ground truth for training and evaluation.}
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{0.4\textwidth}{!}{
\begin{tabular}{|c|c|c|}
\hline
\textbf{Model}          & \textbf{F1 Score (GMRID v1)} & \textbf{F1 Score (GMRID v2)} \\ \hline
Naive Bayes             & 79.93\%                      & 87.61\%                      \\ \hline
Logistic Regression     & 80.39\%                      & \textbf{90.74\%}            \\ \hline
Support Vector Machine  & \textbf{81.63\%}             & 90.08\%                      \\ \hline
Random Forest           & 80.73\%                      & 87.80\%                      \\ \hline
K-nearest Neighbors     & 79.81\%                      & 83.32\%                      \\ \hline
\midrule % Separates the groups
GPT-4o-mini             & 62.85\% (3-shot)             & 93.86\% (1-shot)             \\ \hline
GPT-4o                  & \textbf{65.06\%} (5-shot)             & \textbf{98.76\%} (5-shot)             \\ \hline
\end{tabular}
}
\label{tab:classification_model_evaluation_v2}
\end{table}

The lower performance of the OpenAI models on the GMRID v1 dataset warrants further investigation. To gain a deeper understanding of this issue, we conducted a manual inspection of the results from the top-performing run: \textit{gpt-4o} with 5-shot prompting. Our analysis revealed that, in most instances where the model's output differed from the human-labeled ground truth, the model's classification appeared to be more accurate. Table~\ref{tab:comparison_results} presents a comparison of 10 such cases, where \textit{gpt-4o} provided a more accurate label in 9 out of 10 instances.

\begin{table*}[ht]
\caption{Comparative Analysis of Original Human-Annotated Labels and GPT-4o Generated Labels for Maritime Risk Classification. This table presents 10 sample cases where the GPT-4o model's classification differed from the original human-labeled ground truth. Bolded entries denote the label deemed more accurate after manual review, with GPT-4o being favored in 9 out of 10 cases.}
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.8\textwidth}{!}{
\begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}p{2.3cm}|>{\raggedright\arraybackslash}p{2.3cm}|}
\hline
\textbf{Details (Cleaned)} & \textbf{Original Label} & \textbf{GPT-4o Generated Label} \\
\hline
port captaincy cartagena dimar declared second time hotel la am√©ricas responsible for undue occupation of beach mangrove area; hotel owner comment. & Others & \textbf{Administrative Issue} \\
\hline
government source indicates casualties due to nationwide flooding in kenya. flooding caused by heavy rain impacting horn of africa. government coordinating with kenyan red cross and ngos. & Terrorism & \textbf{Weather} \\
\hline
fire in coal harbour impacted an underground homeless encampment. skytrain service disruptions, evacuation due to smoke. fire controlled, ongoing rail disruption. & Administrative Issue & \textbf{Accident} \\
\hline
three people killed due to poisoning/suffocation in a ship cabin belonging to new energy engineering company.% ltd., shanghai. 
& Terrorism & \textbf{Accident} \\
\hline
china gearing up for chinese new year; factory shutdowns and shipping disruptions expected. higher freight costs anticipated due to increased demand before the holiday. & Worker Strike & \textbf{Administrative Issue} \\
\hline
explosion and fire aboard a vessel at charleston harbor marina; three injured. emergency services on site. & Terrorism & \textbf{Accident} \\
\hline
tropical storm narda caused port closure in manzanillo, mexico; operations resumed. %on september 30. 
& Administrative Issue & \textbf{Weather} \\
\hline
caledonian sleeper service canceled due to planned industrial action by rmt union members. %from september 29 to october 1. 
& Administrative Issue & \textbf{Worker Strike} \\
\hline
8 individuals arrested in new jersey for a multi-million-dollar cargo theft ring. stolen goods included various products from warehouses. & \textbf{Administrative Issue} & Terrorism \\
\hline
heavy rain and strong winds forecasted to hamper operations at the port of colombo. & Administrative Issue & \textbf{Weather} \\
\hline
\end{tabularx}
}
\label{tab:comparison_results}
\end{table*}

The results in Table \ref{tab:classification_model_evaluation_v2} highlight several key observations. Firstly, the v2 dataset, where labels were generated by the \textit{gpt-4o} model, shows improved accuracy for nearly all machine learning models, suggesting that the generated labels are more consistent or better suited to the patterns these models learn.

Furthermore, the OpenAI models, particularly \textit{gpt-4o} with a 5-shot strategy, significantly outperform traditional machine learning models on the v2 dataset, achieving the highest accuracy of 98.76\%. This demonstrates the potential of few-shot prompting with advanced models like \textit{gpt-4o} to deliver superior results, particularly when the ground truth is generated by a similar model.

The noticeable performance differences between the v1 and v2 datasets for the same models suggest potential biases or inconsistencies in human-labeled data that may not align with the patterns these models learn. Consequently, using models like \textit{gpt-4o} for generating ground truth labels could provide a more consistent foundation for training and evaluating classification models.

Overall, these findings suggest that integrating advanced language models with few-shot learning strategies can significantly enhance classification performance, especially when the ground truth data is generated or verified by the same type of models.


\subsection{LLM Classification Performance: Open Source vs. Proprietary Models}

We evaluated the performance of the Llama-3.1 models (8B and 70B) using few-shot prompting and compared their performance against the OpenAI models (GPT-4o and GPT-4o-mini) using the GMRID v2 dataset. Table~\ref{tab:llm_comparison} presents the F1 scores and the ratio of valid categories across different numbers of shots (0, 1, 3, 5, and 10) for each model.

\begin{table}[ht]
\caption{Comprehensive Performance Comparison of Llama-3.1 Models (8B and 70B) and OpenAI Models (GPT-4o-mini and GPT-4o) Across Different Few-Shot Prompting Scenarios. This table presents both F1 scores and Ratio of Valid Categories (RVC) for each model under varying numbers of shots. The the best performance for each model highlighted in bold.}
\centering
\renewcommand{\arraystretch}{1.2}
\resizebox{.45\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Metric} & \textbf{0-shot} & \textbf{1-shot} & \textbf{3-shot} & \textbf{5-shot} & \textbf{10-shot} \\ \hline
\multirow{2}{*}{Llama-3.1-8B} & F1  & 71.51\% & \textbf{87.08\%} & 86.52\% & 86.35\% & 77.28\% \\
 & RVC & 49.43\% & 91.28\% & 92.94\% & 92.68\% & \textbf{99.13}\% \\ \hline
\multirow{2}{*}{Llama-3.1-70B} & F1  & 92.54\% & 93.20\% & 93.83\% & 93.95\% & \textbf{94.01\%} \\
 & RVC & 99.56\% & 99.91\% & \textbf{100.00\%} & \textbf{100.00\%} & \textbf{100.00\%} \\ \hline
\multirow{2}{*}{gpt-4o-mini} & F1  & 92.18\% & \textbf{93.86\%} & 91.92\% & 92.54\% & 92.97\% \\
 & RVC & 99.91\% & \textbf{100.00\%} & \textbf{100.00\%} & 99.91\% & 99.74\% \\ \hline
\multirow{2}{*}{gpt-4o} & F1  & 94.97\% & 97.42\% & 97.46\% & \textbf{98.76\%} & 97.12\% \\
 & RVC & \textbf{100.00\%} & \textbf{100.00\%} & \textbf{100.00\%} & \textbf{100.00\%} & \textbf{100.00\%} \\ \hline
\end{tabular}
}
\label{tab:llm_comparison}
\end{table}

The results reveal several key insights:

\begin{itemize}
    \item \textbf{GPT-4o}: Consistently achieves the highest F1 scores across all numbers of shots, with a peak performance of 98.76\% at 5-shot. It also maintains a perfect 100\% ratio of valid categories across all scenarios, demonstrating the highest level of robustness and reliability among the models tested.
    
    \item \textbf{Llama-3.1-70B}: Shows competitive performance, particularly in maintaining valid categories. It achieves 100\% validity from 3-shot onwards, matching GPT-4o in this aspect. Its F1 scores, while lower than GPT-4o, are consistently above 90\%, peaking at 94.01\% for 10-shot.
    
    \item \textbf{GPT-4o-mini}: Performs well overall but shows minor inconsistencies, particularly in the ratio of valid categories at 0-shot (99.91\%), 5-shot (99.91\%), and 10-shot (99.74\%). Its F1 scores are generally lower than GPT-4o but competitive with Llama-3.1-70B.

    \item \textbf{Llama-3.1-8B}: This model exhibits the highest variability in performance. It demonstrates a substantial improvement in the ratio of valid categories, increasing from 49.43\% at 0-shot to 99.13\% at 10-shot. However, despite the enhancement in F1 scores with additional shots, its performance remains inferior to that of other models. The F1 score peaks at 87.08\% for 1-shot, which is notably lower than the performance of most traditional machine learning models presented in Table~\ref{tab:classification_model_evaluation_v2}. This discrepancy underscores the challenges faced by smaller language models in achieving consistent performance across various few-shot configurations.
\end{itemize}

These findings highlight the impact of model size and architecture on classification performance in few-shot learning contexts. While the proprietary GPT-4o model maintains superior performance across all metrics, the larger Llama-3.1-70B model demonstrates competitive results, particularly in maintaining valid categories. This suggests that open-source models are closing the gap with proprietary ones, especially in terms of output consistency. The performance variations across different shot numbers also underscore the importance of selecting appropriate few-shot prompts for optimal model performance.

