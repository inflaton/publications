@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wang2023cost,
  title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},
  author={Wang, Chi and Liu, Susan Xueqing and Awadallah, Ahmed H},
  journal={arXiv preprint arXiv:2303.04673},
  year={2023}
}

@article{adlakha2023evaluating,
  title={Evaluating correctness and faithfulness of instruction-following models for question answering},
  author={Adlakha, Vaibhav and BehnamGhader, Parishad and Lu, Xing Han and Meade, Nicholas and Reddy, Siva},
  journal={arXiv preprint arXiv:2307.16877},
  year={2023}
}

@article{wang2023pandalm,
  title={PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization},
  author={Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and others},
  journal={arXiv preprint arXiv:2306.05087},
  year={2023}
}

@article{kamalloo2023evaluating,
  title={Evaluating Open-Domain Question Answering in the Era of Large Language Models},
  author={Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles LA and Rafiei, Davood},
  journal={arXiv preprint arXiv:2305.06984},
  year={2023}
}

@article{sharir2020cost,
  title={The cost of training nlp models: A concise overview},
  author={Sharir, Or and Peleg, Barak and Shoham, Yoav},
  journal={arXiv preprint arXiv:2004.08900},
  year={2020}
}

@article{xu2022survey,
  title={A survey of cross-lingual sentiment analysis: Methodologies, models and evaluations},
  author={Xu, Yuemei and Cao, Han and Du, Wanze and Wang, Wenqing},
  journal={Data Science and Engineering},
  volume={7},
  number={3},
  pages={279--299},
  year={2022},
  publisher={Springer}
}

@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@article{cai2022context,
  title={Context-Aware Abbreviation Expansion Using Large Language Models},
  author={Cai, Shanqing and Venugopalan, Subhashini and Tomanek, Katrin and Narayanan, Ajit and Morris, Meredith Ringel and Brenner, Michael P},
  journal={arXiv preprint arXiv:2205.03767},
  year={2022}
}

@article{markus2021role,
  title={The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies},
  author={Markus, Aniek F and Kors, Jan A and Rijnbeek, Peter R},
  journal={Journal of biomedical informatics},
  volume={113},
  pages={103655},
  year={2021},
  publisher={Elsevier}
}

@article{chen2023exploring,
  title={Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study},
  author={Chen, Yi and Wang, Rui and Jiang, Haiyun and Shi, Shuming and Xu, Ruifeng},
  journal={arXiv preprint arXiv:2304.00723},
  year={2023}
}

@article{zheng2023response,
  title={Response Length Perception and Sequence Scheduling: An LLM-Empowered LLM Inference Pipeline},
  author={Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang},
  journal={arXiv preprint arXiv:2305.13144},
  year={2023}
}

@article{zhang2023safetybench,
  title={SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions},
  author={Zhang, Zhexin and Lei, Leqi and Wu, Lindong and Sun, Rui and Huang, Yongkang and Long, Chong and Liu, Xiao and Lei, Xuanyu and Tang, Jie and Huang, Minlie},
  journal={arXiv preprint arXiv:2309.07045},
  year={2023}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{joshi2023deepsumm,
  title={DeepSumm: Exploiting topic models and sequence to sequence networks for extractive text summarization},
  author={Joshi, Akanksha and Fidalgo, Eduardo and Alegre, Enrique and Fern{\'a}ndez-Robles, Laura},
  journal={Expert Systems with Applications},
  volume={211},
  pages={118442},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{tran2019does,
  title={Does BLEU score work for code migration?},
  author={Tran, Ngoc and Tran, Hieu and Nguyen, Son and Nguyen, Hoan and Nguyen, Tien},
  booktitle={2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC)},
  pages={165--176},
  year={2019},
  organization={IEEE}
}

@inproceedings{dey2022evaluating,
  title={Evaluating commit message generation: to BLEU or not to BLEU?},
  author={Dey, Samanta and Vinayakarao, Venkatesh and Gupta, Monika and Dechu, Sampath},
  booktitle={Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
  pages={31--35},
  year={2022}
}

@inproceedings{huang2024evaluation,
  title={Evaluation of Orca 2 against other LLMs for Retrieval Augmented Generation},
  author={Huang, Donghao and Wang, Zhaoxia},
  booktitle={The 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining Workshops (PAKDDW)},
  year={2024},
  Publisher={Springer}
}